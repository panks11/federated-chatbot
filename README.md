
# Federated Text Classification ğŸ§ ğŸ“¡

A modular framework for **centralized** and **federated learning** on **text classification tasks** using **ANN** and **LSTM** models. Supports both **IID** and **non-IID** data distributions across clients.

---

## ğŸš€ Features

- âœ… Centralized & Federated Training
- ğŸ§  Support for ANN and LSTM models
- ğŸ” FedAvg-based aggregation
- ğŸ“Š Client-wise evaluation metrics
- ğŸ”§ Configurable via YAML files
- ğŸ“¦ SNIPS-style dataset loader

---

## ğŸ—‚ Folder Structure

```
federated_text_classification/
â”œâ”€â”€ config/                  # YAML configs (base + client)
â”œâ”€â”€ data/                   # SNIPS-style data and preprocessing
â”œâ”€â”€ models/                 # ANN and LSTM models
â”œâ”€â”€ trainer/                # Centralized and federated trainers
â”œâ”€â”€ utils/                  # Logging, config, metrics, client generation
â”œâ”€â”€ experiments/            # Experiment runners
â””â”€â”€ outputs/                # Logs, checkpoints, and results
```

---

## ğŸ› ï¸ Setup

```bash
git clone https://github.com/your-org/federated-text-classification.git
cd federated-text-classification
pip install -r requirements.txt
```

---

## ğŸ“š Usage

### 1. Preprocess SNIPS Data

```bash
python utils/generate_federated_clients.py --iid       # For IID partitioning
python utils/generate_federated_clients.py             # For non-IID partitioning
```

### 2. Run Centralized Training

```bash
python experiments/run_centralized.py
```

### 3. Run Federated Training

```bash
# Using ANN
python experiments/run_federated_ann.py

# Using LSTM
python experiments/run_federated_lstm.py
```

---

## âš™ï¸ Configuration

All configs are stored in `config/`:

- `base_config.yaml`: global defaults
- `client_{i}_config.yaml`: per-client overrides

You can customize:
- Model type (`ann`, `lstm`)
- Number of clients, rounds, local epochs
- Learning rate, batch size, and more

---

## ğŸ“ˆ Sample Output

```
--- Federated Round 1 ---
Client 0 Accuracy: 0.78
Client 1 Accuracy: 0.82
Average Accuracy After Round 1: 0.80
```

---

## ğŸ“‹ TODO

- [ ] Add support for more NLP datasets
- [ ] Add federated metrics (per-class accuracy, fairness, etc.)
- [ ] Integrate FedProx, FedOpt optimizers
- [ ] Add UI dashboard for training monitoring

---

## ğŸ‘©â€ğŸ’» Authors

- **Pankhuri Kulshrestha** â€“ [@pankhuri-k](https://github.com/pankhuri-k)

---

## ğŸ“„ License

MIT License. See `LICENSE` file for details.
